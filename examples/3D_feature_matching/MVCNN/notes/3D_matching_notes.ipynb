{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3D Feature Matching Progress Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Objective\n",
    "The objective of 3D feature matching is to match 3D features which are known as a multi-view track in SfM pipeline from different point clouds. Based on the point correspondences, merging multiple SfM results is made possible by estimating reliable (similarity) transformations between SfM blocks. As the first yet non-trivial step, learning expressive and discriminative descriptors of 3D tracks is of utmost importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Learn Descriptors\n",
    "Different from pairwise matching, matching 3D tracks is concerned about matching about two groups of views. The representation of each track therefore should integrate the describing capability of various views. The local patches of different views usually undergoes some extent of variance (at least affine transformation). From this perspective, the essential goal of learning is associated with learning the variance of multi-view patches. Generally speaking, the higher nonlinearity the model present, the more abtract activations are produced, and the larger invariance is achieved. For example, in the context of image classification, the very abstract class information like Dog tolerates wide spread of variance of species and appreance of dogs. It's the invariant property that determines the accuracy of classification.\n",
    "\n",
    "#### Progress\n",
    "1. [19/04/2017~20/04/2017] The intial try just takes the siamese network of matchnet and utilize the feature maps on the top of feature tower as the descriptor. The dimension is 4096 for now. Although the weights are trained with a metric network hanging above, the recall of matching with the L2 distance of top feature map is higher than that of matching by the metric network (**52/81(L2) VS 31/81(metric) for stadium**). This makes sense that the overloaded fully connected network is not suitable for patch matching. As inspired by SIFT, a good descriptor can be achieved by aggregating the charateristics of local sub-regions (histogram of gradients in 8 directions are aggregated from 4x4 sub-regions). It is noteworthy that the L2 distances of the 4096-dimensional descriptors are relative. Although filtering the mismatches by absolute distance is impossible here, ratio test is appliable to rule out mismatches. As observed from experimants, outlier matches generally own distance ratio (ratio of distances of closet and second closet neighbors) near 1, while inlier matches can have distance ratio far away from 1 as shown below. For our task of 3D matching, precision of 3D feature correspodences is much more important than recall. Hence, filtering putative matches by ratio test does more good than harm although some inlier matches are also discarded.\n",
    "![Distribution of Distance Ratios](images/dist_ratio.png)\n",
    "![SFZ](images/SFZ.png)\n",
    "![gammon](images/gammon.png)\n",
    "\n",
    "2. **Normalization of feature map** Generally, $C$ channels of output feature maps denote the activations of certain distinctive pattern in input image. Assume $C$ feature functions are composed of cascading convolutions, rectified linear units and poolings, then the top feature map in each channel produced by the feature functions characterizes the feautures that are beyond description due to the high nonlinearity. However, in practice, the activations of some channels are small while activations of some are large. In this case, the 'heavier' channels would dominate the distance measurement and the activations from the 'lighter' channels are wasted. To mitigate the effect, I resort to rescaling methods (MinMax or L2 normalization) to **rectify or inhibit** activations within channels. Based on this, L2 distances are computed and improvements are observed (**1523/4000(MinMax) VS 1491/4000(L2) VS 1416/4000(None) for stadium**). What's more, the positive and negative matches become more separable by their distance:\n",
    "![Distribution of disrances](images/distances.png)\n",
    "\n",
    "3. [20/04/2017~05/05/2017] The objective of this stage is to train the absolute and uniform distance measurement for matching so that correspondences with too large distances can be filtered out uniformly. In order to train absolute distance and preserve contrast between positive and negative pairs simutaneously, constrastive loss is adopted. \n",
    "    * **Single margin VS Bi-margin** Although it is desired that similar pairs exhibit zero distance in space, it is intractable in practice. In order to make life easier, an extra margin_simi is added to distances of postive pairs so that the distance of positive pairs is computed by max(0, margin_simi-dist).\n",
    "A bottleneck is put onto the dim-4096 feature map to reduce the dimension to 512, followed by an L2 normalization layer. For the stadium dataset, **single-margin (0, 0.3)** gives recall **306/4000** which indicates very poor performance. On the contrary, **bi-margin setting (0.2, 0.3)** gives recall **1357/4000**, which is a little bit lower than 2, which is acceptable considering lower dimension. However, there are still two problems. (1) One is that the learned descriptors are still not discriminative enough. Even a lot of mismatches exhibit very small L2 distances near zero. Therefore, mining hard negatives is of utmost importance. (2) The second thing that needs further exploration is the setting of two margins.\n",
    "    * **Margin Setting** To evaluate the influence of margin setting on feature discriminativeness, another setting **(0.1, 0.4)** is adopted and trained based on the model trained from (0.2, 0.3). Numerical comparison of descriptor matching of the earthquake dataset is shown in table below. (0.1, 0.4) outperforms (0.2, 0.3). It implies that separating the margins more widely indeed helps in increasing the discriminative power of descriptors. A simple way of defining the discriminative power of descriptors in matching tasks is that if match is true, \n",
    "$$discriminativeness = log \\frac{D_{sec\\_min}}{D_{min}};$$\n",
    "else if match is false, \n",
    "$$discriminativeness = 0.$$\n",
    "\n",
    "In our practical experiments, measuring discriminativeness of descriptors by ratio is treacherous because of the ambiguous and repetitive local structures. Therefore, we refer to another metric that accumulates the distribution of closet distances of returns to positive and negative queries respectively. If the positives and negatives are separable easily, their distributions should have as less intersection as possible. As comparison, the distributions obtained by our approach and Sift match are shown as below. The numerical measurements, intersection over union (IoU), are 0.062 and 0.186 respectively. The displacement implies that it is much simpler to filter spurious matches by setting threshold to distances of our learned descriptors.\n",
    "\n",
    "![Dist by ours](images/resnet_dist.png)\n",
    "![Dist by sift](images/sift_dist.png)\n",
    "\n",
    "\n",
    "\n",
    "| #TP/ Avg discri | pooling4096 | fc512 | fc512 | mlpconv512 | mlpconv512 | mlpconv512 | resnet | resnet | resnet | resnet | Sift (Cross) | Sift (Mean) | Ground truth |\n",
    "|:--------------------:|:-----------:|:---------:|:---------:|:----------:|:----------:|:----------:|:----------:|----------------------|------------|------------|:------------:|:-----------:|:------------:|\n",
    "| Margin | metric | 0.2, 0.3 | 0.1, 0.4 | 0.1, 0.4 | 0.1, 0.5 | 0.1, 0.6 | 0.1, 0.4 | 0.316, 0.632 (22890) | 0.25 0.75 | 0.2 0.8 | NA |  | NA |\n",
    "| Earthquake | 32/0.678 | 23/1.02 | 34/1.19 | 37/1.520 | 37/1.528 | 37/1.556 |  |  |  |  | 35/1.34 |  | 39 |\n",
    "| SFZ | 469/0.713 | 435/0.979 | 708/1.348 | 872/1.559 | 871/1.577 | 872/1.569 |  |  |  |  | 712/1.355 |  | 990 |\n",
    "| Shaoguan | 2093/0.491 | 1563/0.3 | 2755/0.43 | 3640/0.422 | 3896/0.456 | 3919/0.46 | 4065/0.455 | 4297/0.42 | 4301/0.498 | 4169/0.496 | 3670/0.52 | 3679/0.52 | 5128 |\n",
    "| Temple |  |  |  |  | 1121/1.347 | 1108/1.336 |  |  |  |  | 1063/1.276 |  | 1377 |\n",
    "| test-cyt (viewpoint) |  |  |  |  |  |  |  | 3111/0.956 | 3112/1.106 | 3095/1.105 | 2964/0.925 |  |  |\n",
    "\n",
    "\n",
    "\n",
    "4. [05/05/2017~09/05/2017] **What is the best way of fusing multiple views?** The typical approach to combining feature map of multiviews is called view pooling which extracts the elementwise maximum value from multiple input vectors. The intuitive feeling is that the output maximum feature map records the strongest reponses across the views and thus achieves the invariance to some extent. Let $\\mathbf{x}_1$ and $\\mathbf{x}_2$ denote the feature vectors of two views which are inputs to the convolution layers $C_1$ and $C_2$ that share parameters and $\\mathbf{w}$ and $b$ denote the weights and biase they share. Then after forward the two feature vectors into $C_1$ and $C_2$ followed by the activation function ReLU and then the view pooling layer, the output is \n",
    "$$y = \\max \\left[ \\max{\\left( \\mathbf{x}_1^T \\mathbf{w}+b, 0 \\right)}, \n",
    "\\max{\\left( \\mathbf{x}_2^T \\mathbf{w}+b, 0 \\right)} \\right]$$\n",
    "$$= \\max{\\left( \\mathbf{x}_1^T \\mathbf{w}+b, \\mathbf{x}_2^T \\mathbf{w}+b, 0 \\right)}  .$$\n",
    "_Let $\\vec{x} = [\\mathbf{x}; 1]$ and $\\vec{w} = [\\mathbf{w}; b]$, then_ \n",
    "$$y = \\max{ \\left( \\vec{x}_1^T \\vec{w},  \\vec{x}_2^T \\vec{w}, 0        \\right)}$$\n",
    "The above equation is a maximization over zero and $N$ linear functions w.r.t. parameters $\\vec{w}$, where $N$ is the number of views. However, in practice, matching between mulltiview patches is very complex. Some inlier matches establish large variations (e.g. wide-baseline cases), while some outlier matches could be similar in apperance. These hard cases pose a great challenges to multiview matching. \n",
    "Apparently, this type of manipulation is deficient to handle too complicated cases.\n",
    "\n",
    "To this end, I resort to the more sophiscated **MLP (Multilayer Perceptron)** convolution layer to include higher nonlinearity and expect higher invariance to local transformations. Inspired by Sift descriptor which is constructed by concatenating HOG of 4x4 local sub-windows, the outputs of MLP are directly fed into L2Norm layer as the final representation. As shown in above table, MLP achieves impovement over the descriptor produced by bottleneck layer by a significant margin and even outperforms Sift matcher in some cases. The necessity of discarding fc layer can be supported in two perspectives: \n",
    "1. The fc layer is generally deployed on top of convolution layers in classification tasks. It achieves success because the images of a specific class is highly correlated in spatial space. For example, in an image of human, the relative positions of head, hands and legs are generally fixed. The fc layer combines the layout of feature patterns all over the feature map to provide prediction finally. However, in the task of matching, the layouts of local structures in different images are unrelated. Each intance can even be seen as a distinctive class. Therefore, fc layer is no longer suitable here.\n",
    "2. Fully connected layers are not spatially located anymore compared to convolution layers. Substantially, it takes the input feature map as a one-dimentional vector and results in a large gap of information loss.\n",
    "3. Much less parameters are required. Mlpconv consumes 3x3x192x64+64x32=112640 parameters. Original fc layer consumes 4096x512=2097152 parameters, around 18 times the number of mlpconv.\n",
    "\n",
    "\n",
    "5. **The impact of gaussian weighting**\n",
    "\n",
    "| #TP/ Avg discri | no weighted | weighted(0.5) | weighted(1.0, more contrasted) | weighted(0.5) |\n",
    "|:---------------:|:-----------:|:-------------:|:------------------------------:|:-------------:|\n",
    "|shaoguan         |3873/0.905   |3885/0.918     |3834/0.928                      |3576/0.94      |\n",
    "\n",
    "As shown in above table, applying gaussian weighting explicitly does not make a lot of difference. This may be because that the implicit weighting strategy has been learned during training. The additional weighting is just gilding the lily.\n",
    "\n",
    "\n",
    "6. [09/05/2017~23/05/2017] One main problem with current model is that the model still could not identify the negative matches by their distances very well. My initial analysis is that the network still could not match the complexity of data. Therefore, I'd like to try some variants of current model which adds some modifications to the view fusion phase.\n",
    "\n",
    "| Model zoo        | Description                                                 | #TP (shaoguan) |\n",
    "|:----------------:|:-----------------------------------------------------------:|:--------------:|\n",
    "| View pooling     | Element-wise max of multi-view feature maps                 | 2755           |\n",
    "| mlpconv          | Convolution across all channels of all views                | 3919           |\n",
    "| concat           | Concat feature maps of view pooling and mlpconv             |             \n",
    "| Resnet           | Element-wise sum of view pooling and mlpconv                |\n",
    "\n",
    "*The table above is deprecated* Currently the Resnet franework of the model is adopted to shoulder the view fusion task. It performs well and is elegant and meaningful in intuitive understanding and analysis. The capability of differentiating negative pairs and positive pairs also gets enhanced as indicated in above figures **Distribution of distances**. \n",
    "\n",
    "7. [23/05/2017~] Matching by local visual similarity is insufficient for the challenging task of point cloud registration due to ambiguous and repetitive visual patterns especially when point clouds get large. **Outlier rejection** is critical to ensure consistent registration results. Given a set of point correspondences whose state space is binary, i.e. {0, 1}, we assign a probability to each pair of correspondences. The goal here is to estimate probablities of their states while imposing constraints including:\n",
    "\n",
    "    * Pairwise similarity: distance of descriptors or distance ratio\n",
    "    * Co-visility: Points visible by common camera are considered to be associated. Two pairs of correspondences are regarded to be co-visible if both source points and dest points are co-visible in respective point clouds. This association can be model by conditional depencence of MRF model by adding smooth term to energy function, e.g. KL divergence of state probability distribution of neighboring points.\n",
    "    * Geometric consistency: Let $p$ and $q$ are two points in the same point cloud and $p'$ and $q'$ are two corresponding points in another point cloud. Let $v=p-q$ and $v'=p'-q'$ be the vectors pointing from one point ot another. If both $(p, p')$ and $(q, q')$ are inlier correspondences, the vector pairs $(v, v')$ should follow certain consistency, including angle consistency and scale ratio consistency, since the two point clouds are related by a similarity transform. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
