name: "siamese feature tower"

# ******************head of tower A********************
layer {
  bottom: "data_search"
  top: "conv0"
  name: "conv0"
  type: "Convolution"
  param {
	name: "conv0_w"
	lr_mult : 1
  }
  param {
    name: "conv0_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 24
    pad: 3
    kernel_size: 7
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv0"
  top: "conv0"
  name: "conv0_relu"
  type: "ReLU"
}
layer {
  bottom: "conv0"
  top: "pool0"
  name: "pool0"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  bottom: "pool0"
  top: "conv1"
  name: "conv1"
  type: "Convolution"
  param {
	name: "conv1_w"
	lr_mult : 1
  }
  param {
    name: "conv1_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv1"
  top: "conv1"
  name: "conv1_relu"
  type: "ReLU"
}
layer {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: "Convolution"
  param {
	name: "conv2_w"
	lr_mult : 1
  }
  param {
    name: "conv2_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv2"
  top: "conv2"
  name: "conv2_relu"
  type: "ReLU"
}
layer {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
################# spatial transform 1 ##################
layer {
  bottom: "pool2"
  top: "st1/conv"
  name: "st1/conv"
  type: "Convolution"
  param {
  name: "st1/conv_w"
  lr_mult : 1
  }
  param {
    name: "st1/conv_b"
  lr_mult : 2
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 1
    stride: 1
  weight_filler {
    type : "msra"
  }
  bias_filler {
    type : "constant"
  }
  }
}
layer {
  bottom: "pool2_p"
  top: "st1/conv_p"
  name: "st1/conv_p"
  type: "Convolution"
  param {
  name: "st1/conv_w"
  lr_mult : 1
  }
  param {
    name: "st1/conv_b"
  lr_mult : 2
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 1
    stride: 1
  weight_filler {
    type : "msra"
  }
  bias_filler {
    type : "constant"
  }
  }
}
layer {
  bottom: "st1/conv"
  top: "st1/reshape"
  name: "st1/reshape"
  type: "Reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 8
      dim: 8
    }
  }
}
layer {
  bottom: "st1/reshape"
  bottom: "st1/conv_p"
  top: "st1/concat"
  name: "st1/concat"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "st1/ip1"
  type: "InnerProduct"
  bottom: "st1/concat"
  top: "st1/ip1"
  param {
    lr_mult: 0.0001
  }
  param {
    lr_mult: 0.0001
  }
  inner_product_param {
    num_output: 128
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.001
    }
  }
}
layer {
  bottom: "st1/ip1"
  top: "st1/ip1/bn"
  name: "st1/ip1/bn"
  type: "BatchNorm"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  bottom: "st1/ip1/bn"
  top: "st1/ip1/relu"
  name: "st1/ip1/relu"
  type: "ReLU"
}
layer {
  name: "st1/theta"
  type: "InnerProduct"
  bottom: "st1/ip1/relu"
  top: "st1/theta"
  param {
    lr_mult: 0.0001
  }
  param {
    lr_mult: 0.0001
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "st1/transform"
  type: "SpatialTransformer"
  bottom: "pool2"  # input dim: 16x32x32
  bottom: "st1/theta"
  top: "st1/out"
  st_param {
    # If false, only compute dTheta, DO NOT compute dU
    to_compute_dU: false  
    theta_1_1: 0.5
    theta_1_2: 0.0
    theta_2_1: 0.0
    theta_2_2: 0.5
    output_H: 16  # half size of input 32x32
    output_W: 16
  }
}
################# end of spatial transform 1 ##################
layer {
  bottom: "st1/out"
  top: "conv3"
  name: "conv3"
  type: "Convolution"
  param {
	name: "conv3_w"
	lr_mult : 1
  }
  param {
    name: "conv3_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv3"
  top: "conv3"
  name: "conv3_relu"
  type: "ReLU"
}
layer {
  bottom: "conv3_relu"
  top: "conv4"
  name: "conv4"
  type: "Convolution"
  param {
	name: "conv4_w"
	lr_mult : 1
  }
  param {
    name: "conv4_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv4"
  top: "conv4"
  name: "conv4_relu"
  type: "ReLU"
}
layer {
  bottom: "conv4"
  top: "pool4"
  name: "pool4"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
################# spatial transform 2 ##################
layer {
  bottom: "pool4"
  top: "st2/reshape"
  name: "st2/reshape"
  type: "Reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
      dim: 4
    }
  }
}
layer {
  bottom: "st2/reshape"
  bottom: "pool4_p"
  top: "st2/concat"
  name: "st2/concat"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "st2/ip1"
  type: "InnerProduct"
  bottom: "st2/concat"
  top: "st2/ip1"
  param {
    lr_mult: 0.0001
  }
  param {
    lr_mult: 0.0001
  }
  inner_product_param {
    num_output: 128
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.001
    }
  }
}
layer {
  bottom: "st2/ip1"
  top: "st2/ip1/bn"
  name: "st2/ip1/bn"
  type: "BatchNorm"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  bottom: "st2/ip1/bn"
  top: "st2/ip1/relu"
  name: "st2/ip1/relu"
  type: "ReLU"
}
layer {
  name: "st2/theta"
  type: "InnerProduct"
  bottom: "st2/ip1/relu"
  top: "st2/theta"
  param {
    lr_mult: 0.0001
  }
  param {
    lr_mult: 0.0001
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "st2/transform"
  type: "SpatialTransformer"
  bottom: "pool4"  # input dim: 64x8x8
  bottom: "st2/theta"
  top: "st2/out"
  st_param {
    # If false, only compute dTheta, DO NOT compute dU
    to_compute_dU: false  
    theta_1_1: 0.5
    theta_1_2: 0.0
    theta_2_1: 0.0
    theta_2_2: 0.5
    output_H: 4  # half size of input 64x4x4
    output_W: 4
  }
}
################# end of spatial transform 2 ##################

# ******************end of tower A*********************


# ******************head of tower P********************
layer {
  bottom: "data_template"
  top: "conv0_p"
  name: "conv0_p"
  type: "Convolution"
  param {
	name: "conv0_w"
	lr_mult : 1
  }
  param {
    name: "conv0_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 24
    pad: 3
    kernel_size: 7
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv0_p"
  top: "conv0_p"
  name: "conv0_relu_p"
  type: "ReLU"
}
layer {
  bottom: "conv0_p"
  top: "pool0_p"
  name: "pool0_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  bottom: "pool0_p"
  top: "conv1_p"
  name: "conv1_p"
  type: "Convolution"
  param {
	name: "conv1_w"
	lr_mult : 1
  }
  param {
    name: "conv1_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv1_p"
  top: "conv1_p"
  name: "conv1_relu_p"
  type: "ReLU"
}
layer {
  bottom: "conv1_p"
  top: "pool1_p"
  name: "pool1_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  bottom: "pool1_p"
  top: "conv2_p"
  name: "conv2_p"
  type: "Convolution"
  param {
	name: "conv2_w"
	lr_mult : 1
  }
  param {
    name: "conv2_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv2_p"
  top: "conv2_p"
  name: "conv2_relu_p"
  type: "ReLU"
}
layer {
  bottom: "conv2_p"
  top: "pool2_p"
  name: "pool2_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  bottom: "pool2_p"
  top: "conv3_p"
  name: "conv3_p"
  type: "Convolution"
  param {
	name: "conv3_w"
	lr_mult : 1
  }
  param {
    name: "conv3_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv3_p"
  top: "conv3_p"
  name: "conv3_relu_p"
  type: "ReLU"
}
layer {
  bottom: "conv3_relu_p"
  top: "conv4_p"
  name: "conv4_p"
  type: "Convolution"
  param {
	name: "conv4_w"
	lr_mult : 1
  }
  param {
    name: "conv4_b"
	lr_mult : 2
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
	weight_filler {
		type : "msra"
	}
	bias_filler {
		type : "constant"
	}
  }
}
layer {
  bottom: "conv4_p"
  top: "conv4_p"
  name: "conv4_relu_p"
  type: "ReLU"
}
layer {
  bottom: "conv4_p"
  top: "pool4_p"
  name: "pool4_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
# ******************end of tower P*********************

input: "data_search"
input_dim: 10
input_dim: 1
input_dim: 256
input_dim: 256

input: "data_template"
input_dim: 10
input_dim: 1
input_dim: 64
input_dim: 64

output: "st2/out"
output_dim: 10
output_dim: 64
output_dim: 4
output_dim: 4
# 64x4x4 = 1024

output: "pool4_p"
output_dim: 10
output_dim: 64
output_dim: 4
output_dim: 4
# 64x4x4 = 1024